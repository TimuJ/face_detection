{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from skimage import io\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = 'data/Menpo_68p/test/aflw__face_44574.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_bbox(file_path: str) -> torch.FloatTensor:\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    img = io.imread(file_path)\n",
    "    dets, _, _ = detector.run(img, 1, -1)\n",
    "    try:\n",
    "        face_bbox = torch.tensor([dets[0].left(), dets[0].top(), dets[0].right(), dets[0].bottom()], dtype=torch.float)\n",
    "    except IndexError:\n",
    "        print(f\"Following face wasn't recognized {file_path}\")\n",
    "        face_bbox = [[None]*4]\n",
    "    return face_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 81.,  53., 210., 182.])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_bbox(path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_points = 'data/Menpo_68p/test/aflw__face_39844.pts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_keypoint(jpg_path: str) -> torch.FloatTensor:\n",
    "    #assume .pts file is in the same fir as .jpg\n",
    "    pts_name = jpg_path[:-4] + '.pts'\n",
    "    with open(pts_name) as f:\n",
    "        lines = f.readlines()\n",
    "        if lines[0].startswith('version'):  # to support different formats\n",
    "            lines = lines[3:-1]\n",
    "        mat = np.fromstring(''.join(lines), sep=' ')\n",
    "        mat_tensor = torch.tensor((mat.reshape((68, 2))), dtype=torch.float)\n",
    "        visibility = torch.ones([68, 1], dtype=torch.float)\n",
    "        keypoint = torch.cat((mat_tensor, visibility), dim=1)                    \n",
    "    return keypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([68, 3])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_keypoint(path_to_points).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceLandmarksDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dir_to_jpgs: str, transform=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            dir_to_folder (string): Path to folder with .jpg and .pts files.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = dir_to_jpgs\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        for idx, fname in enumerate(os.listdir(self.root_dir)):\n",
    "            cur_path = os.path.join(self.root_dir, fname)\n",
    "            if cur_path.endswith('.jpg'):\n",
    "                self.images.append(cur_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "        bbox = face_bbox(img_path) # [x1, y1, x2, y2]\n",
    "        keypoints = read_keypoint(img_path) # (FloatTensor[K, 3]) format K : [x, y, visibility]\n",
    "        labels = torch.tensor([1], dtype=torch.int64)\n",
    "        img = img / 255 # normalize values  \n",
    "        img_height, img_width, _ = img.shape\n",
    "        if bbox == [[None]*4]:\n",
    "            # ловим неопределяшки от dlib -> пустые таргеты\n",
    "            bbox = torch.empty((0, 4), dtype=torch.float)\n",
    "            keypoints2d = torch.empty((0, 21, 3), dtype=torch.float)\n",
    "            labels = torch.tensor([0], dtype=torch.int64)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img) # to tensor, from shape (H, W, C) -> (C, H, W)\n",
    "        img = img.to(torch.float)\n",
    "\n",
    "        target = {\n",
    "            'path': img_path,\n",
    "            'boxes': bbox,\n",
    "            'keypoints': keypoints,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_folder = 'data/Menpo_68p/test/'\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "trainset = FaceLandmarksDataset(dir_to_jpgs=path_to_folder, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.1725, 0.1569, 0.1725,  ..., 0.9804, 1.0000, 0.3490],\n",
       "          [0.1686, 0.1529, 0.1647,  ..., 0.9804, 1.0000, 0.3490],\n",
       "          [0.1608, 0.1451, 0.1569,  ..., 0.9804, 1.0000, 0.3490],\n",
       "          ...,\n",
       "          [0.8863, 0.9961, 0.9882,  ..., 0.9804, 1.0000, 0.3490],\n",
       "          [0.9490, 0.9922, 0.9490,  ..., 0.9804, 1.0000, 0.3490],\n",
       "          [1.0000, 0.9647, 0.9255,  ..., 0.9804, 1.0000, 0.3490]],\n",
       " \n",
       "         [[0.0784, 0.0667, 0.0627,  ..., 0.9804, 1.0000, 0.3490],\n",
       "          [0.0745, 0.0588, 0.0627,  ..., 0.9804, 1.0000, 0.3490],\n",
       "          [0.0667, 0.0510, 0.0549,  ..., 0.9804, 1.0000, 0.3490],\n",
       "          ...,\n",
       "          [0.8353, 0.9569, 0.9529,  ..., 0.9804, 1.0000, 0.3490],\n",
       "          [0.9059, 0.9569, 0.9216,  ..., 0.9804, 1.0000, 0.3490],\n",
       "          [0.9686, 0.9373, 0.8941,  ..., 0.9804, 1.0000, 0.3490]],\n",
       " \n",
       "         [[0.0314, 0.0118, 0.0157,  ..., 0.9804, 1.0000, 0.3490],\n",
       "          [0.0275, 0.0118, 0.0118,  ..., 0.9804, 1.0000, 0.3490],\n",
       "          [0.0196, 0.0039, 0.0039,  ..., 0.9804, 1.0000, 0.3490],\n",
       "          ...,\n",
       "          [0.8039, 0.9216, 0.9255,  ..., 0.9804, 1.0000, 0.3490],\n",
       "          [0.8824, 0.9294, 0.8980,  ..., 0.9804, 1.0000, 0.3490],\n",
       "          [0.9490, 0.9137, 0.8824,  ..., 0.9804, 1.0000, 0.3490]]]),\n",
       " {'path': 'data/Menpo_68p/test/aflw__face_44574.jpg',\n",
       "  'boxes': tensor([ 81.,  53., 210., 182.]),\n",
       "  'keypoints': tensor([[ 78.8370, 101.3250,   1.0000],\n",
       "          [ 79.5410, 117.9430,   1.0000],\n",
       "          [ 82.2130, 134.7370,   1.0000],\n",
       "          [ 87.1760, 150.1040,   1.0000],\n",
       "          [ 98.3650, 162.7900,   1.0000],\n",
       "          [113.6590, 171.6390,   1.0000],\n",
       "          [129.8920, 177.6100,   1.0000],\n",
       "          [146.0460, 181.7540,   1.0000],\n",
       "          [159.4850, 180.9960,   1.0000],\n",
       "          [168.8570, 174.9360,   1.0000],\n",
       "          [172.8050, 164.4070,   1.0000],\n",
       "          [177.0550, 153.2030,   1.0000],\n",
       "          [181.1860, 142.1130,   1.0000],\n",
       "          [184.3680, 131.5020,   1.0000],\n",
       "          [186.7510, 120.4950,   1.0000],\n",
       "          [185.8850, 109.7510,   1.0000],\n",
       "          [183.2220,  99.4950,   1.0000],\n",
       "          [108.4950,  93.5160,   1.0000],\n",
       "          [116.3960,  86.6340,   1.0000],\n",
       "          [128.0400,  83.7270,   1.0000],\n",
       "          [140.4880,  85.7060,   1.0000],\n",
       "          [148.2540,  89.9480,   1.0000],\n",
       "          [162.7800,  88.5530,   1.0000],\n",
       "          [168.6970,  85.2310,   1.0000],\n",
       "          [174.7590,  82.3020,   1.0000],\n",
       "          [179.6020,  83.3210,   1.0000],\n",
       "          [182.6600,  87.1440,   1.0000],\n",
       "          [158.5250, 100.1190,   1.0000],\n",
       "          [161.5710, 107.6100,   1.0000],\n",
       "          [164.4560, 113.9210,   1.0000],\n",
       "          [167.6710, 120.4950,   1.0000],\n",
       "          [148.7800, 130.0110,   1.0000],\n",
       "          [157.2380, 130.6860,   1.0000],\n",
       "          [164.4960, 132.1150,   1.0000],\n",
       "          [169.6180, 129.8200,   1.0000],\n",
       "          [174.2500, 126.9030,   1.0000],\n",
       "          [118.5670, 103.2460,   1.0000],\n",
       "          [126.4100, 100.1160,   1.0000],\n",
       "          [133.2890,  99.4420,   1.0000],\n",
       "          [139.6240, 104.6790,   1.0000],\n",
       "          [133.2170, 105.7480,   1.0000],\n",
       "          [126.0810, 105.2390,   1.0000],\n",
       "          [164.8200, 101.4170,   1.0000],\n",
       "          [170.8600,  96.7430,   1.0000],\n",
       "          [176.6320,  96.3860,   1.0000],\n",
       "          [181.1310,  98.8680,   1.0000],\n",
       "          [177.5540, 101.6590,   1.0000],\n",
       "          [172.2110, 102.6900,   1.0000],\n",
       "          [129.2440, 145.7240,   1.0000],\n",
       "          [142.2940, 140.3650,   1.0000],\n",
       "          [155.2210, 136.7680,   1.0000],\n",
       "          [161.7600, 137.3890,   1.0000],\n",
       "          [167.3300, 135.2610,   1.0000],\n",
       "          [173.9110, 136.3730,   1.0000],\n",
       "          [176.7980, 139.6450,   1.0000],\n",
       "          [173.8980, 151.3550,   1.0000],\n",
       "          [168.7400, 158.4400,   1.0000],\n",
       "          [162.2080, 160.4250,   1.0000],\n",
       "          [155.1060, 161.1980,   1.0000],\n",
       "          [142.5850, 156.5250,   1.0000],\n",
       "          [133.1650, 145.6530,   1.0000],\n",
       "          [155.1530, 143.0280,   1.0000],\n",
       "          [161.6430, 142.5870,   1.0000],\n",
       "          [167.1440, 140.9340,   1.0000],\n",
       "          [173.8780, 140.2490,   1.0000],\n",
       "          [168.2760, 150.3920,   1.0000],\n",
       "          [162.1720, 152.5580,   1.0000],\n",
       "          [155.3580, 153.2030,   1.0000]]),\n",
       "  'labels': tensor([1])})"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    # initialize the model\n",
    "    model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False,\n",
    "                                                                   num_keypoints=68, \n",
    "                                                                   )\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
